# Aurora GPU Mesh - Unified Monitoring System

**Production-ready GPU mesh with health monitoring, keepalive, auto-recovery, and alerts**

## ğŸš€ Features

- âœ… **Continuous Health Monitoring** - Checks all GPU nodes every 30s
- âœ… **Automatic Keepalive** - Maintains connections and attempts recovery
- âœ… **Auto-Recovery** - Automatically restarts failed Ollama instances
- âœ… **Real-time Alerts** - Warning/Error/Critical alerts with graduated severity
- âœ… **Centralized Logging** - All activity logged to `/tmp/aurora-gpu-mesh/gpu-mesh.log`
- âœ… **Performance Tracking** - Success rates, response times, uptime scores
- âœ… **Live Dashboard** - Console dashboard updates every minute
- âœ… **Systemd Service** - Runs as user service with auto-restart on failure
- âœ… **Statistics Reports** - Detailed stats every 5 minutes

## ğŸ“Š Monitored GPU Nodes

1. **Local Ollama** (`localhost:11434`) - Primary local GPU
2. **Iceland/RunPod Tunnel** (`localhost:8080`) - SSH tunnel to remote GPU
3. _(Expandable to more nodes)_

## ğŸ”§ Installation

### Quick Start

```bash
cd /home/allan/aurora-ai-robbiverse/services/gpu-mesh

# Install as systemd service
bash install-mesh-service.sh
```

### Manual Installation

```bash
# 1. Install service file
mkdir -p ~/.config/systemd/user/
cp aurora-gpu-mesh.service ~/.config/systemd/user/
systemctl --user daemon-reload

# 2. Enable and start
systemctl --user enable aurora-gpu-mesh
systemctl --user start aurora-gpu-mesh

# 3. Set up log rotation (optional)
bash setup-log-rotation.sh
```

## ğŸ“ˆ Monitoring & Status

### Quick Status Check

```bash
bash check-mesh-status.sh
```

Output:
```
ğŸ¯ AURORA GPU MESH - QUICK STATUS CHECK
========================================

âœ… GPU Mesh Service: RUNNING

ğŸ“Š GPU Node Status:

ğŸŸ¢ Local Ollama (11434): HEALTHY - 8 models
ğŸŸ¢ Iceland/RunPod (8080): HEALTHY - 12 models

ğŸ“ Recent Log Activity:
[Recent health checks and alerts]
```

### Service Status

```bash
# Check service
systemctl --user status aurora-gpu-mesh

# View live logs
journalctl --user -u aurora-gpu-mesh -f

# View mesh logs
tail -f /tmp/aurora-gpu-mesh/gpu-mesh.log
```

### Service Control

```bash
# Start
systemctl --user start aurora-gpu-mesh

# Stop
systemctl --user stop aurora-gpu-mesh

# Restart
systemctl --user restart aurora-gpu-mesh

# Disable auto-start
systemctl --user disable aurora-gpu-mesh
```

## ğŸ¯ How It Works

### Health Monitoring Loop (every 30s)

1. Checks all GPU nodes in parallel
2. Measures response time
3. Fetches model list
4. Updates node status (Healthy/Degraded/Unhealthy/Offline)
5. Logs results

### Keepalive Loop (every 60s)

1. Identifies unhealthy/offline nodes
2. Attempts automatic recovery:
   - **Local Ollama**: Checks process, restarts if needed
   - **Tunnels**: Logs for manual intervention
3. Sends recovery alerts

### Alert System

**Alert Levels:**
- ğŸŸ¢ **INFO**: Node recovered, routine events
- ğŸŸ¡ **WARNING**: Node degraded (1-2 failures)
- ğŸ”´ **ERROR**: Node unhealthy (3+ failures)
- ğŸš¨ **CRITICAL**: Node offline (5+ failures)

### Statistics Reporter (every 5 minutes)

Logs comprehensive stats:
- Node status and health scores
- Success rates
- Response times
- Model counts
- Overall mesh health percentage

### Live Dashboard (every 60s)

Console display showing:
- Real-time node status
- Response times and success rates
- Uptime scores
- Recent alerts (last 5 min)

## ğŸ“Š Node Status States

| State | Icon | Description | Action |
|-------|------|-------------|--------|
| **Healthy** | ğŸŸ¢ | 0 failures, responding | Normal operation |
| **Degraded** | ğŸŸ¡ | 1-2 failures | Warning logged |
| **Unhealthy** | ğŸ”´ | 3-4 failures | Error logged, recovery attempt |
| **Offline** | âš« | 5+ failures | Critical alert, recovery attempt |
| **Unknown** | âšª | Not checked yet | Initial state |

## ğŸ” Monitoring Metrics

For each node:
- **Status**: Current health state
- **Response Time**: API response latency (ms)
- **Success Rate**: % of successful requests
- **Uptime Score**: Overall health score (0-100)
- **Models**: Number of loaded models
- **Consecutive Failures**: Failure counter
- **Last Check**: Timestamp of last health check

## ğŸš¨ Alert Examples

```
â„¹ï¸ [INFO] Local Ollama: Node recovered! Response time: 45ms, Models: 8

âš ï¸ [WARNING] Iceland/RunPod (Tunnel): Node degraded: Timeout (10s)

âŒ [ERROR] Local Ollama: Node UNHEALTHY: Connection error: ClientError

ğŸš¨ [CRITICAL] Iceland/RunPod (Tunnel): Node OFFLINE after 5 failures: Timeout (10s)
```

## ğŸ“ Log Files

### Main Log
`/tmp/aurora-gpu-mesh/gpu-mesh.log`
- All health checks
- Status changes
- Alerts
- Recovery attempts
- Statistics reports

### Systemd Journal
```bash
journalctl --user -u aurora-gpu-mesh -f
```
- Service start/stop
- System-level events
- Crash reports

### Log Rotation
After running `setup-log-rotation.sh`:
- Rotates daily
- Keeps 7 days of logs
- Compresses old logs
- Max 100MB per log file

## ğŸ”§ Configuration

Edit `unified_gpu_mesh.py` to customize:

```python
# Check intervals
self.check_interval = 30  # Health check frequency (seconds)
self.keepalive_interval = 60  # Keepalive frequency (seconds)
self.stats_interval = 300  # Stats report frequency (seconds)

# Alert thresholds
self.max_consecutive_failures = 3  # Unhealthy threshold
self.critical_failure_threshold = 5  # Offline threshold

# Add more GPU nodes
self.nodes['vengeance'] = GPUNode(
    name='Vengeance GPU',
    url='http://vengeance.local:11434',
    port=11434
)
```

## ğŸ§ª Testing

### Test Node Failure

```bash
# Stop Ollama to trigger alerts
pkill -f "ollama serve"

# Watch mesh detect failure and attempt recovery
tail -f /tmp/aurora-gpu-mesh/gpu-mesh.log
```

Expected behavior:
1. After 1-2 checks: **WARNING** - Node degraded
2. After 3 checks: **ERROR** - Node unhealthy
3. Keepalive loop attempts restart
4. If successful: **INFO** - Node recovered

### Test Full Mesh

```bash
# Check all nodes are healthy
bash check-mesh-status.sh

# View live dashboard
journalctl --user -u aurora-gpu-mesh -f | grep "DASHBOARD"
```

## ğŸ¯ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Unified GPU Mesh Coordinator        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Health Monitor Loop (30s)       â”‚ â”‚
â”‚  â”‚   - Check all nodes in parallel   â”‚ â”‚
â”‚  â”‚   - Update status & metrics       â”‚ â”‚
â”‚  â”‚   - Send alerts on changes        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Keepalive Loop (60s)            â”‚ â”‚
â”‚  â”‚   - Identify failed nodes         â”‚ â”‚
â”‚  â”‚   - Attempt auto-recovery         â”‚ â”‚
â”‚  â”‚   - Restart Ollama if needed      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Stats Reporter (5min)           â”‚ â”‚
â”‚  â”‚   - Generate statistics           â”‚ â”‚
â”‚  â”‚   - Log comprehensive report      â”‚ â”‚
â”‚  â”‚   - Calculate mesh health         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Dashboard (60s)                 â”‚ â”‚
â”‚  â”‚   - Display live status           â”‚ â”‚
â”‚  â”‚   - Show recent alerts            â”‚ â”‚
â”‚  â”‚   - Update metrics                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Alert Processor                 â”‚ â”‚
â”‚  â”‚   - Manage alert queue            â”‚ â”‚
â”‚  â”‚   - Log with severity levels      â”‚ â”‚
â”‚  â”‚   - Track alert history           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€
              â”‚         â”‚         â”‚
              â–¼         â–¼         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Local  â”‚ â”‚Tunnel  â”‚ â”‚Venge-  â”‚
         â”‚ Ollama â”‚ â”‚:8080   â”‚ â”‚ance    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Next Steps

### Expand the Mesh

Add Vengeance GPU:
```python
# In unified_gpu_mesh.py
self.nodes['vengeance'] = GPUNode(
    name='Vengeance GPU',
    url='http://10.59.98.X:11434',  # Your Vengeance Nebula IP
    port=11434
)
```

### Email Alerts (Optional)

Add SMTP configuration for critical alerts:
```python
# In send_alert() method
if level == AlertLevel.CRITICAL:
    await self.send_email_alert(alert)
```

### Workload Distribution

Integrate with your AI services to route requests to healthy nodes:
```python
# Get best available node
healthy_nodes = [n for n in mesh.nodes.values() if n.status == NodeStatus.HEALTHY]
best_node = max(healthy_nodes, key=lambda n: n.uptime_score)
```

## ğŸ“ Troubleshooting

### Service won't start

```bash
# Check logs
journalctl --user -u aurora-gpu-mesh -n 50

# Test script directly
python3 unified_gpu_mesh.py
```

### Nodes always showing offline

```bash
# Test node connectivity
curl http://localhost:11434/api/tags
curl http://localhost:8080/api/tags

# Check if Ollama is running
ps aux | grep ollama

# Check SSH tunnel
ps aux | grep "ssh.*8080"
```

### Logs not rotating

```bash
# Test logrotate
sudo logrotate -f /etc/logrotate.d/aurora-gpu-mesh

# Check logrotate config
cat /etc/logrotate.d/aurora-gpu-mesh
```

## ğŸ“š Files

- `unified_gpu_mesh.py` - Main mesh coordinator
- `aurora-gpu-mesh.service` - Systemd service file
- `install-mesh-service.sh` - Installation script
- `check-mesh-status.sh` - Quick status checker
- `setup-log-rotation.sh` - Log rotation setup
- `README.md` - This file

## ğŸ‰ Success Criteria

âœ… Service runs continuously without crashes
âœ… All healthy nodes show ğŸŸ¢ status
âœ… Failed nodes trigger alerts within 2 minutes
âœ… Auto-recovery restarts failed Ollama instances
âœ… Logs track all activity
âœ… Dashboard updates every minute
âœ… Statistics reports every 5 minutes

---

**Built for Allan's GPU mesh - Production ready! ğŸš€**
