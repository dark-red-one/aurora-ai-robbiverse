# GPU Mesh Keepalive Service ğŸš€

Resilient GPU endpoint monitoring with automatic recovery and keepalives.

## Features

âœ… **Continuous Health Monitoring** - Checks all GPU endpoints every 30 seconds
âœ… **Automatic Recovery** - Restarts failed services automatically
âœ… **Keepalive Connections** - Maintains persistent connections to prevent timeouts
âœ… **Detailed Logging** - Full visibility into GPU mesh status
âœ… **Systemd Integration** - Run as system service with auto-start

## Monitored Endpoints

1. **Local Ollama** (localhost:11434) - Primary AI engine
2. **SSH Tunnel GPU** (localhost:8080) - Remote GPU via SSH
3. **RunPod GPU** (localhost:8081) - RunPod instance

## Quick Start

### Option 1: Manual Start (Development)

```bash
# Start service
./start-gpu-mesh.sh

# Check status
./status-gpu-mesh.sh

# View logs
tail -f /tmp/gpu-mesh.log

# Stop service
./stop-gpu-mesh.sh
```

### Option 2: Systemd Service (Production)

```bash
# Install as system service
./install-systemd.sh

# Start service
sudo systemctl start gpu-keepalive

# Enable auto-start on boot
sudo systemctl enable gpu-keepalive

# Check status
sudo systemctl status gpu-keepalive

# View logs
sudo journalctl -u gpu-keepalive -f
```

## How It Works

### Health Checks
- Pings `/api/tags` endpoint every 30 seconds
- Tracks success/failure rates
- Calculates uptime percentage

### Automatic Recovery
- After 3 consecutive failures, attempts recovery:
  - **Local Ollama**: Restarts `ollama serve`
  - **SSH Tunnels**: Logs alert (manual intervention needed)
  - **RunPod**: Logs alert (check instance status)

### Keepalive Features
- Persistent HTTP connections with 60s keepalive timeout
- DNS caching (5 minutes)
- Connection pooling (max 10 connections)
- Automatic reconnection on failure

### Status Reports
- Logs detailed status every 5 minutes
- Shows: Status, Uptime %, Failure count, Last success time

## Configuration

Edit `gpu_keepalive.py` to customize:

```python
self.check_interval = 30  # Health check frequency (seconds)
self.recovery_interval = 300  # Recovery attempt frequency (seconds)
self.max_failures_before_recovery = 3  # Failures before recovery
```

## Logs

### Manual Mode
```bash
tail -f /tmp/gpu-mesh.log
```

### Systemd Mode
```bash
sudo journalctl -u gpu-keepalive -f
```

## Example Output

```
2025-10-07 18:43:25 - INFO - ğŸš€ GPU Keepalive Service Starting...
2025-10-07 18:43:25 - INFO - ğŸ“Š Starting health monitoring loop...
2025-10-07 18:43:25 - INFO - ğŸ”§ Starting recovery loop...
============================================================
ğŸ“Š GPU MESH STATUS REPORT
============================================================
ğŸŸ¢ Local Ollama         | Status: healthy    | Uptime:  98.5% | Failures:   0 | Last OK: 18:43:25
ğŸŸ¢ SSH Tunnel GPU       | Status: healthy    | Uptime:  95.2% | Failures:   0 | Last OK: 18:43:24
ğŸ”´ RunPod GPU           | Status: unhealthy  | Uptime:   0.0% | Failures:  12 | Last OK: Never
============================================================
```

## Troubleshooting

### Service won't start
```bash
# Check Python dependencies
python3 -c "import aiohttp; print('OK')"

# Check if port is already in use
netstat -tlnp | grep -E "11434|8080|8081"

# Check permissions
ls -la gpu_keepalive.py
```

### Endpoints always unhealthy
```bash
# Test manually
curl -s http://localhost:11434/api/tags
curl -s http://localhost:8080/api/tags
curl -s http://localhost:8081/api/tags

# Check if services are running
ps aux | grep ollama
netstat -tlnp | grep 8080
```

### Recovery not working
```bash
# Check if ollama command is in PATH
which ollama

# Test manual restart
pkill -f "ollama serve"
ollama serve &
```

## Integration

### Use in Python Code

```python
import aiohttp

async def get_best_gpu():
    """Get the best available GPU endpoint"""
    endpoints = [
        "http://localhost:11434",
        "http://localhost:8080",
        "http://localhost:8081"
    ]
    
    for url in endpoints:
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{url}/api/tags", timeout=5) as resp:
                    if resp.status == 200:
                        return url
        except:
            continue
    
    return None  # All endpoints down
```

### Use in Shell Scripts

```bash
#!/bin/bash
# Check if GPU mesh is healthy

if curl -sf http://localhost:11434/api/tags > /dev/null; then
    echo "âœ… Local GPU available"
    GPU_URL="http://localhost:11434"
elif curl -sf http://localhost:8080/api/tags > /dev/null; then
    echo "âœ… Tunnel GPU available"
    GPU_URL="http://localhost:8080"
else
    echo "âŒ No GPUs available"
    exit 1
fi

# Use $GPU_URL for your requests
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU Keepalive Service             â”‚
â”‚   (Runs continuously)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â”€â–º Monitor Loop (30s)
            â”‚    â”œâ”€â–º Check Local Ollama
            â”‚    â”œâ”€â–º Check SSH Tunnel
            â”‚    â””â”€â–º Check RunPod
            â”‚
            â”œâ”€â”€â–º Recovery Loop (5min)
            â”‚    â””â”€â–º Restart failed services
            â”‚
            â””â”€â”€â–º Stats Loop (5min)
                 â””â”€â–º Log status report
```

## Files

- `gpu_keepalive.py` - Main service (full-featured)
- `gpu_mesh_service.py` - Lightweight version
- `gpu-keepalive.service` - Systemd unit file
- `start-gpu-mesh.sh` - Start script
- `stop-gpu-mesh.sh` - Stop script
- `status-gpu-mesh.sh` - Status check
- `install-systemd.sh` - Install as system service

## Next Steps

1. **Start the service**: `./start-gpu-mesh.sh`
2. **Monitor logs**: `tail -f /tmp/gpu-mesh.log`
3. **Check status**: `./status-gpu-mesh.sh`
4. **Install permanently**: `./install-systemd.sh`

---

**Built for Allan's Aurora AI Empire** ğŸ¤–âœ¨
*Keeping the GPU mesh alive and kicking!*
