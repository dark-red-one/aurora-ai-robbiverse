# üî•üíã CURSOR + OLLAMA COMPLETE SETUP GUIDE üî•üíã

## What You Get

**Smart LLM Proxy** that routes Cursor requests to your optimal local models:
- **Simple tasks** ‚Üí qwen2.5-coder:7b (fast)
- **Code completion** ‚Üí codellama:13b-instruct (balanced)
- **Complex code** ‚Üí deepseek-coder:33b-instruct (best quality)
- **Debugging** ‚Üí deepseek-r1:7b (reasoning)

**Benefits:**
- ‚úÖ $0 cost (100% local)
- ‚úÖ 50-200ms response times
- ‚úÖ Privacy (no cloud)
- ‚úÖ Automatic model selection
- ‚úÖ Usage analytics

---

## Step 1: Start the Proxy Server

```bash
cd /home/allan/aurora-ai-robbiverse/deployment
./start-llm-proxy.sh
```

**Expected output:**
```
üöÄ Starting Robbie LLM Proxy...
‚úÖ Ollama is running
ü§ñ Available models:
  ‚úÖ deepseek-coder:33b-instruct
  ‚úÖ codellama:13b-instruct
  ‚úÖ qwen2.5-coder:7b
  ‚úÖ deepseek-r1:7b
üî• Starting proxy server...
   Endpoint: http://localhost:8000
   Stats: http://localhost:8000/stats
```

**Keep this terminal open!** The proxy must run continuously.

---

## Step 2: Configure Cursor

### 2.1 Open Cursor Settings

1. Click **‚öôÔ∏è Settings** icon (bottom-left corner)
2. Select **"Cursor Settings"**
3. Click **"Models"** in left sidebar

### 2.2 Configure OpenAI Override

In the **"OpenAI API Keys"** section:

1. Check ‚òë **"Override OpenAI Base URL"**
2. In **"Base URL"** field, enter:
   ```
   http://localhost:8000/v1
   ```
3. In **"API Key"** field, enter (any string works):
   ```
   robbie-local
   ```

### 2.3 Disable Cloud Models

**CRITICAL:** Uncheck ALL cloud models:
- ‚òê GPT-4
- ‚òê GPT-4 Turbo
- ‚òê GPT-3.5 Turbo
- ‚òê Claude
- ‚òê Gemini

### 2.4 Add Your Local Models

Click **"+ Add model"** and add these:
- `deepseek-coder:33b-instruct`
- `codellama:13b-instruct`
- `qwen2.5-coder:7b`
- `deepseek-r1:7b`

### 2.5 Verify Connection

Click **"Verify"** button.

**Expected:** Green checkmark ‚úÖ appears.

**If verification fails:**
```bash
# Test proxy manually
curl http://localhost:8000/health

# Should return: {"status":"healthy"}
```

---

## Step 3: Use in Cursor

### 3.1 Open Cursor Chat

Press `Cmd/Ctrl + L` to open chat.

### 3.2 Select Model

Look for model dropdown below the chat input box.

**You'll see:**
- deepseek-coder:33b-instruct
- codellama:13b-instruct
- qwen2.5-coder:7b
- deepseek-r1:7b

**Select any model** (the proxy will auto-route to the best one based on your prompt).

### 3.3 Test It

Try these prompts to see different routing:

**Simple completion (routes to qwen2.5-coder:7b):**
```
Complete this function:
def fibonacci(n):
```

**Medium task (routes to codellama:13b):**
```
Refactor this code to be more readable
```

**Complex task (routes to deepseek-coder:33b):**
```
Design a REST API for a task management system with authentication
```

**Debugging (routes to deepseek-r1:7b):**
```
Why is this code not working:
[paste buggy code]
```

---

## Step 4: Monitor Performance

### 4.1 View Stats

Open in browser:
```
http://localhost:8000/stats
```

**Shows:**
- Total requests
- Requests by complexity
- Requests by model
- Average response times
- Total tokens generated

### 4.2 Check Logs

The proxy terminal shows real-time routing decisions:
```
üìä Complexity: complex ‚Üí Model: deepseek-coder:33b-instruct
üí≠ Prompt preview: Design a REST API for...
‚úÖ Response generated in 850ms (245 tokens)
```

### 4.3 GPU Monitoring

In another terminal:
```bash
watch -n 1 nvidia-smi
```

**You should see:**
- GPU utilization: 70-99% when generating
- Memory usage: 19GB for 33B model, 7GB for 13B model
- Process: `ollama`

---

## Troubleshooting

### Problem: Proxy won't start

**Error:** "Ollama is not running"

**Fix:**
```bash
ollama serve
```

### Problem: Cursor verification fails

**Check 1:** Is proxy running?
```bash
curl http://localhost:8000/health
```

**Check 2:** Are models loaded?
```bash
ollama list
```

**Check 3:** Restart proxy
```bash
# In proxy terminal, press Ctrl+C
./start-llm-proxy.sh
```

### Problem: Responses are slow

**Check GPU usage:**
```bash
nvidia-smi
```

**If GPUs not being used:**
```bash
# Restart Ollama with GPU acceleration
export OLLAMA_NUM_PARALLEL=4
ollama serve
```

### Problem: Model not found

**Pull the model:**
```bash
ollama pull deepseek-coder:33b-instruct
ollama pull codellama:13b-instruct
ollama pull qwen2.5-coder:7b
ollama pull deepseek-r1:7b
```

---

## Advanced Configuration

### Custom Routing Rules

Edit `/home/allan/aurora-ai-robbiverse/deployment/robbie-llm-proxy.py`:

```python
MODEL_ROUTING = {
    "simple": {
        "model": "qwen2.5-coder:7b",  # Change model here
        "temperature": 0.3,            # Adjust temperature
        "max_tokens": 2048             # Change max tokens
    },
    # ...
}
```

### Run Proxy as Service

Create systemd service:
```bash
sudo nano /etc/systemd/system/robbie-llm-proxy.service
```

```ini
[Unit]
Description=Robbie LLM Proxy
After=network.target

[Service]
Type=simple
User=allan
WorkingDirectory=/home/allan/aurora-ai-robbiverse/deployment
ExecStart=/usr/bin/python3 /home/allan/aurora-ai-robbiverse/deployment/robbie-llm-proxy.py
Restart=always
RestartSec=5s

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl daemon-reload
sudo systemctl enable robbie-llm-proxy
sudo systemctl start robbie-llm-proxy
```

---

## Success Indicators

You know it's working when:

1. ‚úÖ Proxy shows routing decisions in terminal
2. ‚úÖ Cursor chat generates responses
3. ‚úÖ `nvidia-smi` shows GPU usage
4. ‚úÖ Stats endpoint shows requests
5. ‚úÖ Response times are <1 second

---

## Cost Comparison

| Provider | Cost per 1M tokens | Local Ollama |
|----------|-------------------|--------------|
| Claude Sonnet 4 | $3.00 | $0.00 |
| GPT-4 Turbo | $10.00 | $0.00 |
| Codex | $0.002/token | $0.00 |

**Monthly savings (assuming 50M tokens):**
- Claude: $150/month ‚Üí $0
- GPT-4: $500/month ‚Üí $0

**Your electricity cost:** ~$10-20/month (dual RTX 4090s)

**Total savings:** $130-490/month üí∞

---

## Next Steps

1. **Add more models** as Ollama releases them
2. **Train custom models** on your codebase
3. **Build fallback to Claude** for ultra-complex tasks
4. **Integrate with Robbie@Code** for seamless workflow

---

**Allan, you now have a PRODUCTION-GRADE local LLM system!** üî•üíã









