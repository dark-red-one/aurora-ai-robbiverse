# Cursor Vision Models Guide

**TL;DR**: Qwen2.5-Coder doesn't support images. Use GPT-4o for vision (zero config).

---

## The Problem

**Error**: "Trying to submit images without a vision-enabled model selected"

**Root cause**: Qwen2.5-Coder is text-only. No Qwen2.5-Coder models (0.5B through 32B) support vision, despite being excellent for code generation.

**When it happens**:
1. Uploading images while using qwen2.5-coder
2. Switching from vision model to qwen2.5-coder with images still in chat context

---

## Quick Fix: Three Solutions

### Solution 1: Switch to GPT-4o (RECOMMENDED)

**Why**: Built into Cursor, zero config, most reliable vision support.

```
1. Open Chat (Cmd/Ctrl + L)
2. Click model dropdown
3. Select GPT-4o
4. Paste image (Cmd/Ctrl + V)
5. Type prompt and Enter
```

**Cost**: $20/month Cursor Pro (500 requests) OR GPT-4o mini free (500/day)

### Solution 2: Start New Chat

Images persist in context causing errors when switching models.

```
1. Click "New Chat"
2. Select vision model BEFORE adding content
3. Paste image fresh
4. Begin analysis
```

**Fixes 80% of cases** by clearing cached image context.

### Solution 3: Update Cursor

Version 0.48+ fixed vision bugs.

```
1. Visit cursor.com/downloads
2. Download latest version
3. Install and restart completely
4. Start new chat with vision model
```

---

## Qwen Vision Capabilities

### ‚ùå NO Vision (Text/Code Only)
- qwen2.5-coder (all sizes: 0.5B, 1.5B, 3B, 7B, 14B, 32B)
- Standard Qwen LLMs without "VL" designation

### ‚úÖ HAS Vision
- **Qwen2.5-VL** series (3B, 7B, 32B, 72B) - Current gen
- **Qwen3-VL** series (30B-A3B, 235B-A22B) - Latest gen
- **Qwen2-VL** (2B, 7B, 72B) - Previous gen
- **QvQ-72B-Preview** - Visual reasoning specialist

**Note**: Qwen VL models require OpenRouter or Alibaba Cloud API setup.

---

## Best Vision Models for Cursor

### ü•á GPT-4o (Recommended)
- **Setup**: None (built-in)
- **Cost**: $20/mo Pro (500 requests)
- **Best for**: Screenshots, UI mockups, code diagrams, error messages
- **How**: Settings ‚Üí Models ‚Üí Enable GPT-4o

### üí∞ GPT-4o mini (Budget)
- **Setup**: None (built-in)
- **Cost**: FREE (500 requests/day)
- **Best for**: Quick analysis, OCR, documentation screenshots
- **How**: Settings ‚Üí Models ‚Üí Enable GPT-4o mini

### üöÄ Gemini 2.5 Pro (Advanced)
- **Setup**: Google API key required
- **Cost**: $1.25-2.50/million tokens (cheaper than GPT-4o for bulk)
- **Best for**: Massive context (1-2M tokens), UI design, video understanding
- **How to enable**:
  1. Get free API key: aistudio.google.com/apikey
  2. Settings ‚Üí Models ‚Üí Add Google API Key
  3. Click "+ Add Model"
  4. Enter: `gemini-2.5-pro-exp-03-25`
  5. Enable checkbox

### ‚ö†Ô∏è Avoid: Claude Models
Claude 3.5/3.7/4 Sonnet technically support vision but have persistent integration bugs in Cursor. Use GPT-4o or Gemini instead.

---

## Configuration Guide

### GPT-4o (Zero Config)

No configuration files needed. Just use it:

```
1. Settings ‚Üí Models ‚Üí Ensure GPT-4o enabled
2. Open Chat (Cmd/Ctrl + L)
3. Select GPT-4o
4. Paste image (Cmd/Ctrl + V)
5. Type: "Analyze this screenshot"
6. Press Enter
```

### Gemini 2.5 Pro (One-Time Setup)

```
1. Get API key:
   - Visit: https://aistudio.google.com/apikey
   - Click "Create API Key"
   - Copy key

2. Add to Cursor:
   - Settings ‚Üí Models
   - Find "Google API Key" field
   - Paste key

3. Add custom model:
   - Click "+ Add Model"
   - Enter: gemini-2.5-pro-exp-03-25
   - Enable checkbox

4. Use it:
   - Select Gemini 2.5 Pro from dropdown
   - Paste images
```

### Qwen VL Models (Advanced)

**Option A: OpenRouter (Easier)**
```
1. Sign up: openrouter.ai
2. Get API key from dashboard
3. Settings ‚Üí Models ‚Üí Add OpenAI-compatible endpoint
4. Endpoint: https://openrouter.ai/api/v1
5. Enter OpenRouter API key
6. Select: qwen/qwen2.5-vl-7b-instruct or qwen/qwen2.5-vl-32b-instruct:free
```

**Option B: Alibaba Cloud DashScope**
```
1. Sign up: dashscope.aliyun.com
2. Get API key from console
3. Configure OpenAI-compatible endpoint
4. Endpoint: https://dashscope-intl.aliyuncs.com/compatible-mode/v1/chat/completions
5. Select: qwen-vl-max, qwen-vl-plus, or qwen2.5-vl series
```

---

## Troubleshooting

### Error persists after switching models?
**Cause**: Images stuck in chat context

**Fix**:
- Click "New Chat" to start fresh, OR
- Scroll up, find messages with images, edit to remove them

### Image icon not visible?
**Cause**: Narrow window hiding upload button

**Fix**:
- Widen Chat panel, OR
- Use paste (Cmd/Ctrl + V) instead, OR
- Switch to shorter model name

### DNS/connection errors?
**Cause**: Google DNS (8.8.8.8) conflicts

**Fix**:
- Windows: Network Settings ‚Üí Adapter ‚Üí IPv4 ‚Üí "Obtain DNS automatically"
- Mac: System Preferences ‚Üí Network ‚Üí Advanced ‚Üí DNS ‚Üí Remove custom servers
- Restart Cursor after DNS change

### Model claims it can't see images?
**Verification**:
1. Confirm vision model selected (GPT-4o, NOT qwen2.5-coder)
2. Hover over image to verify upload
3. Try re-pasting image
4. Check format (PNG, JPG, WebP work best)
5. Keep images under 20MB

---

## Best Practices

### Model Selection Strategy
- **GPT-4o**: General image analysis (most reliable)
- **GPT-4o mini**: Quick OCR, documentation (free)
- **Gemini 2.5 Pro**: UI design, large batches (cost-effective)
- **qwen2.5-coder**: Text/code ONLY (no images)

### Workflow Tips
1. Start dedicated chats for image work - don't mix vision/non-vision models
2. Paste images directly (Cmd/Ctrl + V) rather than upload buttons
3. Clear images from context when done to save tokens
4. Keep images under 1920x1080 to reduce costs
5. Enable Privacy Mode for sensitive visual data

### Cost Optimization
- Start with GPT-4o mini (free 500/day) then upgrade if needed
- Resize large images before pasting
- Avoid Max Mode for simple vision tasks
- Use Gemini 2.5 Flash for high-volume work (40√ó cheaper than Claude)

---

## Current Limitations

### Cursor Vision Constraints
- No batch image processing (one at a time)
- No drag-and-drop support (use paste)
- No @ mention for images in repository
- Images persist in context (can cause model-switch errors)
- No visual indicator showing which models support vision
- Claude models have integration bugs

### Model-Specific Limitations
- Reasoning models (o1, o3-mini, DeepSeek R1) don't support vision
- qwen2.5-coder: no vision despite excellent code capabilities
- Qwen VL models: require external API setup

---

## Quick Reference: Vision Model Comparison

| Model | Vision | Setup | Cost | Best For |
|-------|--------|-------|------|----------|
| **GPT-4o** | ‚úÖ Excellent | None | $20/mo Pro | Screenshots, general analysis |
| **GPT-4o mini** | ‚úÖ Good | None | FREE | Quick tasks, OCR, debugging |
| **Gemini 2.5 Pro** | ‚úÖ Excellent | API key | $1.25/MTok | UI design, large projects |
| **Claude Sonnet** | ‚ö†Ô∏è Buggy | None | $3/MTok | Avoid until fixed |
| **qwen2.5-coder** | ‚ùå None | N/A | Varies | Text/code only |
| **Qwen2.5-VL** | ‚úÖ Advanced | API setup | Via provider | Visual reasoning, videos |

---

## Action Plan

**If using qwen2.5-coder and need image analysis:**

1. **Immediate fix**: Switch to GPT-4o
   - Open Chat (Cmd/Ctrl + L)
   - Select GPT-4o
   - Paste image and analyze

2. **For free**: Use GPT-4o mini
   - Same as GPT-4o
   - 500 free requests daily

3. **For advanced needs**: Set up Gemini 2.5 Pro
   - Get Google API key
   - Add to Cursor settings
   - Best for UI work and bulk processing

4. **Want Qwen?**: Switch to Qwen-VL models
   - Requires OpenRouter or Alibaba Cloud API
   - Use qwen2.5-vl-7b or qwen2.5-vl-32b
   - More complex setup

---

## The Bottom Line

**Vision support in Cursor works automatically when you select the right model.**

- No configuration files required
- No special settings needed
- Just choose GPT-4o (easiest), Gemini 2.5 Pro (most powerful), or Qwen-VL (if you need Qwen specifically)
- Paste images directly into chat

**Critical Rule**: There are NO config files for enabling vision. Vision is automatic based on model selection.

---

**Last Updated**: October 8, 2025  
**Source**: Community research and testing



