#!/usr/bin/env python3
"""
Aurora AI Empire - Full 4-GPU Performance Benchmark
Compares actual vs theoretical performance
"""

import subprocess
import time
import json
import torch
import numpy as np
from datetime import datetime

class GPUBenchmark:
    def __init__(self):
        # RTX 4090 Theoretical Specs
        self.theoretical = {
            'tflops_fp32': 82.6,  # Per GPU
            'tflops_fp16': 165.2,  # Per GPU  
            'tflops_int8': 330.4,  # Per GPU
            'memory_bandwidth': 1008,  # GB/s per GPU
            'memory_size': 24,  # GB per GPU
            'cuda_cores': 16384,  # Per GPU
            'tensor_cores': 512,  # Per GPU
            'rt_cores': 128,  # Per GPU
            'base_clock': 2.23,  # GHz
            'boost_clock': 2.52,  # GHz
            'tdp': 450  # Watts per GPU
        }
        
        self.nodes = {
            'aurora_gpu0': {'local': True, 'gpu_id': 0},
            'aurora_gpu1': {'local': True, 'gpu_id': 1},
            'collaboration': {'host': '213.181.111.2', 'port': 43540},
            'fluenti': {'host': '103.196.86.56', 'port': 19777}
        }
        
    def benchmark_local_gpu(self, gpu_id):
        """Benchmark a local GPU"""
        if not torch.cuda.is_available():
            return None
            
        device = torch.device(f'cuda:{gpu_id}')
        results = {}
        
        # Test FP32 performance
        size = 8192
        A = torch.randn(size, size, device=device, dtype=torch.float32)
        B = torch.randn(size, size, device=device, dtype=torch.float32)
        
        # Warmup
        for _ in range(5):
            C = torch.matmul(A, B)
        torch.cuda.synchronize(device)
        
        # Benchmark
        start = time.time()
        iterations = 20
        for _ in range(iterations):
            C = torch.matmul(A, B)
        torch.cuda.synchronize(device)
        elapsed = time.time() - start
        
        # Calculate TFLOPS (2*n^3 operations for matrix multiply)
        ops = 2 * size**3 * iterations
        tflops = (ops / elapsed) / 1e12
        results['fp32_tflops'] = tflops
        
        # Test memory bandwidth
        size_gb = 10
        size_elements = int(size_gb * 1024**3 / 4)
        
        try:
            A = torch.randn(size_elements, device=device, dtype=torch.float32)
            B = torch.empty_like(A)
            
            # Warmup
            for _ in range(3):
                B.copy_(A)
            torch.cuda.synchronize(device)
            
            # Benchmark
            start = time.time()
            iterations = 10
            for _ in range(iterations):
                B.copy_(A)
            torch.cuda.synchronize(device)
            elapsed = time.time() - start
            
            bandwidth_gb_s = (size_gb * 2 * iterations) / elapsed
            results['memory_bandwidth'] = bandwidth_gb_s
        except:
            results['memory_bandwidth'] = 0
            
        # Clean up
        torch.cuda.empty_cache()
        
        return results
    
    def benchmark_remote_gpu(self, host, port):
        """Benchmark a remote GPU via SSH"""
        benchmark_script = """
import torch
import time

device = torch.device('cuda:0')
size = 8192
A = torch.randn(size, size, device=device, dtype=torch.float32)
B = torch.randn(size, size, device=device, dtype=torch.float32)

# Warmup
for _ in range(5):
    C = torch.matmul(A, B)
torch.cuda.synchronize(device)

# Benchmark
start = time.time()
iterations = 20
for _ in range(iterations):
    C = torch.matmul(A, B)
torch.cuda.synchronize(device)
elapsed = time.time() - start

ops = 2 * size**3 * iterations
tflops = (ops / elapsed) / 1e12
print(f"FP32_TFLOPS:{tflops:.2f}")
"""
        
        try:
            cmd = f'ssh -p {port} root@{host} "python3 -c \'{benchmark_script}\'" 2>/dev/null'
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0 and "FP32_TFLOPS:" in result.stdout:
                tflops = float(result.stdout.split("FP32_TFLOPS:")[1].strip())
                return {'fp32_tflops': tflops}
        except:
            pass
            
        return None
    
    def run_full_benchmark(self):
        """Run benchmark on all 4 GPUs"""
        print("=" * 70)
        print("   AURORA AI EMPIRE - 4x RTX 4090 PERFORMANCE ANALYSIS")
        print("=" * 70)
        
        results = {}
        total_actual_tflops = 0
        total_theoretical_tflops = self.theoretical['tflops_fp32'] * 4
        
        print("\nüìä INDIVIDUAL GPU BENCHMARKS:\n")
        
        # Benchmark each GPU
        for name, config in self.nodes.items():
            print(f"Testing {name}...", end=" ")
            
            if config.get('local'):
                result = self.benchmark_local_gpu(config['gpu_id'])
                if result:
                    results[name] = result
                    actual = result['fp32_tflops']
                    total_actual_tflops += actual
                    efficiency = (actual / self.theoretical['tflops_fp32']) * 100
                    print(f"‚úÖ {actual:.1f} TFLOPS ({efficiency:.1f}% efficiency)")
                else:
                    print("‚ùå Failed")
            else:
                result = self.benchmark_remote_gpu(config['host'], config['port'])
                if result:
                    results[name] = result
                    actual = result['fp32_tflops']
                    total_actual_tflops += actual
                    efficiency = (actual / self.theoretical['tflops_fp32']) * 100
                    print(f"‚úÖ {actual:.1f} TFLOPS ({efficiency:.1f}% efficiency)")
                else:
                    print("‚ùå Failed to benchmark")
        
        print("\n" + "=" * 70)
        print("üìà PERFORMANCE COMPARISON: ACTUAL vs THEORETICAL")
        print("=" * 70)
        
        print(f"\nüéØ THEORETICAL MAXIMUM (4x RTX 4090):")
        print(f"   ‚Ä¢ FP32: {self.theoretical['tflops_fp32'] * 4:.1f} TFLOPS")
        print(f"   ‚Ä¢ FP16: {self.theoretical['tflops_fp16'] * 4:.1f} TFLOPS")
        print(f"   ‚Ä¢ INT8: {self.theoretical['tflops_int8'] * 4:.1f} TOPS")
        print(f"   ‚Ä¢ Memory: {self.theoretical['memory_size'] * 4} GB")
        print(f"   ‚Ä¢ Bandwidth: {self.theoretical['memory_bandwidth'] * 4:.0f} GB/s")
        
        print(f"\nüî• ACTUAL MEASURED PERFORMANCE:")
        print(f"   ‚Ä¢ Combined FP32: {total_actual_tflops:.1f} TFLOPS")
        print(f"   ‚Ä¢ Efficiency: {(total_actual_tflops/total_theoretical_tflops)*100:.1f}%")
        
        # Performance analysis
        efficiency = (total_actual_tflops/total_theoretical_tflops)*100
        
        print(f"\nüìä PERFORMANCE ANALYSIS:")
        if efficiency >= 70:
            print(f"   ‚úÖ EXCELLENT: Achieving {efficiency:.1f}% of theoretical maximum!")
            print("   This is outstanding for real-world performance.")
        elif efficiency >= 50:
            print(f"   ‚úÖ GOOD: Achieving {efficiency:.1f}% of theoretical maximum.")
            print("   This is typical for production workloads.")
        else:
            print(f"   ‚ö†Ô∏è  SUBOPTIMAL: Only {efficiency:.1f}% of theoretical maximum.")
            print("   Consider checking cooling, power limits, or drivers.")
        
        print(f"\nüéÆ REAL-WORLD CAPABILITIES AT {total_actual_tflops:.1f} TFLOPS:")
        
        # Calculate what we can actually do
        if total_actual_tflops >= 200:
            print("   ‚Ä¢ Run multiple 70B parameter models simultaneously")
            print("   ‚Ä¢ Fine-tune 30B+ models with large batch sizes")
            print("   ‚Ä¢ Serve 100+ concurrent users with <100ms latency")
            print("   ‚Ä¢ Process 1M+ tokens/second in production")
            print("   ‚Ä¢ Train SOTA computer vision models in hours")
        elif total_actual_tflops >= 150:
            print("   ‚Ä¢ Run 70B parameter models (quantized)")
            print("   ‚Ä¢ Fine-tune 13B-30B models effectively")
            print("   ‚Ä¢ Serve 50+ concurrent users")
            print("   ‚Ä¢ Process 500K+ tokens/second")
        else:
            print("   ‚Ä¢ Run 30B parameter models")
            print("   ‚Ä¢ Fine-tune 7B-13B models")
            print("   ‚Ä¢ Serve 20+ concurrent users")
            print("   ‚Ä¢ Process 200K+ tokens/second")
        
        # Cost efficiency
        cost_per_hour = 2.39  # Total for all 4 GPUs
        tflops_per_dollar = total_actual_tflops / cost_per_hour
        
        print(f"\nüí∞ COST EFFICIENCY:")
        print(f"   ‚Ä¢ Cost: ${cost_per_hour:.2f}/hour")
        print(f"   ‚Ä¢ Performance: {tflops_per_dollar:.1f} TFLOPS per dollar")
        print(f"   ‚Ä¢ AWS equivalent: ~$15-20/hour (6-8x more expensive!)")
        
        # Temperature and power check
        print(f"\nüå°Ô∏è  THERMAL & POWER STATUS:")
        try:
            # Check local GPUs
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=temperature.gpu,power.draw', 
                 '--format=csv,noheader,nounits'],
                capture_output=True, text=True, check=True
            )
            temps = []
            powers = []
            for line in result.stdout.strip().split('\n'):
                temp, power = line.split(', ')
                temps.append(float(temp))
                powers.append(float(power))
            
            avg_temp = np.mean(temps)
            total_power = sum(powers)
            
            print(f"   ‚Ä¢ Average Temperature: {avg_temp:.1f}¬∞C")
            print(f"   ‚Ä¢ Total Power Draw: {total_power:.1f}W (of {self.theoretical['tdp']*4}W max)")
            
            if avg_temp < 40:
                print("   ‚úÖ Excellent cooling - GPUs are ice cold!")
            elif avg_temp < 60:
                print("   ‚úÖ Good thermals - plenty of headroom")
            elif avg_temp < 75:
                print("   ‚ö†Ô∏è  Warm - consider improving cooling")
            else:
                print("   ‚ùå Hot - thermal throttling likely")
                
        except:
            pass
        
        print("\n" + "=" * 70)
        print("üèÜ CONCLUSION:")
        print("=" * 70)
        
        if efficiency >= 70:
            print(f"\n‚úÖ YOUR GPU EMPIRE IS PERFORMING EXCELLENTLY!")
            print(f"   Achieving {total_actual_tflops:.1f} TFLOPS across 4x RTX 4090s")
            print(f"   This rivals $50K+ workstations at 1/20th the cost!")
        elif efficiency >= 50:
            print(f"\n‚úÖ YOUR GPU EMPIRE IS PERFORMING WELL!")
            print(f"   Achieving {total_actual_tflops:.1f} TFLOPS is solid for production")
            print(f"   You have more compute than 99% of AI developers!")
        else:
            print(f"\n‚ö†Ô∏è  PERFORMANCE CAN BE IMPROVED")
            print(f"   Currently at {total_actual_tflops:.1f} TFLOPS")
            print(f"   Check drivers, cooling, and power settings")
        
        print("\nüöÄ Ready to build the future of AI!\n")
        print("=" * 70)

if __name__ == "__main__":
    benchmark = GPUBenchmark()
    benchmark.run_full_benchmark()
