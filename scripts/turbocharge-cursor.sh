#!/bin/bash
# TURBOCHARGE CURSOR WITH AURORA AI EMPIRE
# Connects Cursor to GPU mesh, memory system, and business automation

set -e

echo "üöÄ TURBOCHARGING CURSOR WITH AURORA AI EMPIRE"
echo "=============================================="

# Configuration
GPU_MESH_ENDPOINT="http://209.170.80.132:8002"
RUNPOD_IP="209.170.80.132"
RUNPOD_PORT="13323"
AURORA_TOWN_IP="45.32.194.172"

echo "üîó GPU Mesh: $GPU_MESH_ENDPOINT"
echo "‚ö° RunPod: $RUNPOD_IP:$RUNPOD_PORT"
echo "üèõÔ∏è Aurora Town: $AURORA_TOWN_IP"

# Step 1: Deploy Ollama on RunPod with GPU acceleration
echo ""
echo "üìä Step 1: Deploying Ollama on RunPod with GPU acceleration..."

ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 root@$RUNPOD_IP -p $RUNPOD_PORT << 'RUNPODEOF'
echo "üöÄ Installing Ollama on RunPod with GPU acceleration..."

# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Configure Ollama for GPU mesh
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_GPU_LAYERS=999
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_KEEP_ALIVE=24h

# Create Ollama service
cat > /etc/systemd/system/ollama-gpu-mesh.service << 'SERVICEEOF'
[Unit]
Description=Ollama GPU Mesh Service
After=network.target

[Service]
Type=simple
User=root
Environment=OLLAMA_HOST=0.0.0.0:11434
Environment=OLLAMA_GPU_LAYERS=999
Environment=OLLAMA_FLASH_ATTENTION=1
Environment=OLLAMA_KEEP_ALIVE=24h
ExecStart=/usr/local/bin/ollama serve
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
SERVICEEOF

# Start Ollama
systemctl daemon-reload
systemctl enable ollama-gpu-mesh
systemctl start ollama-gpu-mesh
sleep 5

# Pull models
echo "üì¶ Pulling AI models..."
ollama pull qwen2.5:7b
ollama pull qwen2.5:14b
ollama pull codellama:13b
ollama pull llama3.1:8b

echo "‚úÖ Ollama deployed with GPU acceleration"
echo "üß™ Testing Ollama..."
ollama list
RUNPODEOF

# Step 2: Create Cursor GPU Mesh Integration
echo ""
echo "üîß Step 2: Creating Cursor GPU Mesh Integration..."

cat > /tmp/cursor-gpu-mesh-integration.py << 'CURSOREOF'
#!/usr/bin/env python3
"""
Cursor GPU Mesh Integration
Connects Cursor to RunPod GPU mesh for maximum performance
"""

import json
import requests
import subprocess
from pathlib import Path

class CursorGPUMeshIntegration:
    def __init__(self):
        self.cursor_settings_path = Path.home() / "Library/Application Support/Cursor/User/settings.json"
        self.gpu_mesh_endpoint = "http://209.170.80.132:8002"
        self.runpod_ollama = "http://209.170.80.132:11434"
        
    def check_gpu_mesh(self):
        """Check GPU mesh status"""
        try:
            response = requests.get(f"{self.gpu_mesh_endpoint}/mesh/status", timeout=10)
            data = response.json()
            print(f"‚úÖ GPU Mesh: {data['mesh_status']} - {data['active_nodes']} nodes")
            print(f"‚úÖ Available VRAM: {data['available_vram_gb']}GB")
            return True
        except Exception as e:
            print(f"‚ùå GPU Mesh error: {e}")
            return False
    
    def check_runpod_ollama(self):
        """Check RunPod Ollama"""
        try:
            response = requests.get(f"{self.runpod_ollama}/api/tags", timeout=10)
            data = response.json()
            models = [model['name'] for model in data.get('models', [])]
            print(f"‚úÖ RunPod Ollama: {len(models)} models available")
            for model in models:
                print(f"   ‚Ä¢ {model}")
            return True
        except Exception as e:
            print(f"‚ùå RunPod Ollama error: {e}")
            return False
    
    def configure_cursor_for_gpu_mesh(self):
        """Configure Cursor to use GPU mesh"""
        print("üîß Configuring Cursor for GPU mesh...")
        
        # Load current settings
        settings = {}
        if self.cursor_settings_path.exists():
            with open(self.cursor_settings_path, 'r') as f:
                settings = json.load(f)
        
        # GPU Mesh configuration
        gpu_mesh_config = {
            # Primary AI provider - RunPod Ollama
            "cursor.ai.provider": "ollama",
            "cursor.ai.model": "qwen2.5:7b",
            "cursor.ai.url": self.runpod_ollama,
            "cursor.ai.local": False,
            "cursor.ai.remote": True,
            
            # Ollama settings
            "cursor.ollama.enabled": True,
            "cursor.ollama.url": self.runpod_ollama,
            "cursor.ollama.model": "qwen2.5:7b",
            "cursor.ollama.fallback": "qwen2.5:14b",
            "cursor.ollama.codeModel": "codellama:13b",
            "cursor.ollama.reasoningModel": "qwen2.5:14b",
            
            # Performance settings
            "cursor.ai.timeout": 60000,
            "cursor.ai.maxTokens": 8192,
            "cursor.ai.temperature": 0.7,
            "cursor.ai.stream": True,
            
            # Robbie personality
            "cursor.ai.personality": "robbie",
            "cursor.ai.systemPrompt": "You are Robbie, Allan's AI executive assistant and strategic partner at TestPilot CPG. You are direct, curious, honest, pragmatic, and revenue-focused. Always think three steps ahead and focus on what moves the needle. You have access to 673,780 tokens/min GPU performance.",
            
            # GPU Mesh integration
            "cursor.gpu.mesh.enabled": True,
            "cursor.gpu.mesh.endpoint": self.gpu_mesh_endpoint,
            "cursor.gpu.mesh.performance": "673780_tokens_per_min",
            "cursor.gpu.mesh.faultTolerance": True,
            
            # Disable cloud providers
            "cursor.ai.cloud.enabled": False,
            "cursor.ai.anthropic.enabled": False,
            "cursor.ai.openai.enabled": False,
            "cursor.ai.gpt4.enabled": False,
            "cursor.ai.claude.enabled": False,
            
            # Aurora integration
            "cursor.aurora.enabled": True,
            "cursor.aurora.memory": True,
            "cursor.aurora.business": True,
            "cursor.aurora.widgets": True,
        }
        
        # Merge settings
        settings.update(gpu_mesh_config)
        
        # Save settings
        self.cursor_settings_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.cursor_settings_path, 'w') as f:
            json.dump(settings, f, indent=4)
        
        print(f"‚úÖ Cursor configured for GPU mesh: {self.cursor_settings_path}")
        return True
    
    def create_cursor_extension(self):
        """Create Cursor extension for GPU mesh integration"""
        print("üì¶ Creating Cursor GPU mesh extension...")
        
        extension_dir = Path.home() / ".cursor/extensions/cursor-gpu-mesh"
        extension_dir.mkdir(parents=True, exist_ok=True)
        
        # Package.json
        package_json = {
            "name": "cursor-gpu-mesh",
            "displayName": "Cursor GPU Mesh Integration",
            "description": "Connect Cursor to Aurora GPU mesh for maximum performance",
            "version": "1.0.0",
            "publisher": "robbie",
            "engines": {"vscode": "^1.85.0"},
            "categories": ["Other"],
            "activationEvents": ["onStartupFinished"],
            "main": "./extension.js",
            "contributes": {
                "configuration": {
                    "title": "Cursor GPU Mesh",
                    "properties": {
                        "cursor.gpu.mesh.enabled": {
                            "type": "boolean",
                            "default": True,
                            "description": "Enable GPU mesh integration"
                        },
                        "cursor.gpu.mesh.endpoint": {
                            "type": "string",
                            "default": "http://209.170.80.132:8002",
                            "description": "GPU mesh endpoint"
                        },
                        "cursor.ollama.url": {
                            "type": "string",
                            "default": "http://209.170.80.132:11434",
                            "description": "RunPod Ollama URL"
                        }
                    }
                }
            }
        }
        
        with open(extension_dir / "package.json", 'w') as f:
            json.dump(package_json, f, indent=2)
        
        # Extension.js
        extension_js = '''const vscode = require('vscode');
const { exec } = require('child_process');

class GPUMeshProvider {
    constructor() {
        this.gpuMeshEndpoint = vscode.workspace.getConfiguration('cursor.gpu.mesh').get('endpoint');
        this.ollamaUrl = vscode.workspace.getConfiguration('cursor.ollama').get('url');
    }
    
    async generateResponse(prompt, model = 'qwen2.5:7b') {
        return new Promise((resolve, reject) => {
            const payload = {
                model: model,
                prompt: prompt,
                stream: false,
                options: {
                    temperature: 0.7,
                    max_tokens: 8192
                }
            };
            
            const curlCmd = `curl -s -X POST ${this.ollamaUrl}/api/generate -H "Content-Type: application/json" -d '${JSON.stringify(payload)}'`;
            
            exec(curlCmd, (error, stdout, stderr) => {
                if (error) {
                    reject(error);
                    return;
                }
                
                try {
                    const response = JSON.parse(stdout);
                    resolve(response.response);
                } catch (e) {
                    reject(e);
                }
            });
        });
    }
    
    async getGPUMeshStatus() {
        return new Promise((resolve, reject) => {
            exec(`curl -s ${this.gpuMeshEndpoint}/mesh/status`, (error, stdout, stderr) => {
                if (error) {
                    reject(error);
                    return;
                }
                
                try {
                    const status = JSON.parse(stdout);
                    resolve(status);
                } catch (e) {
                    reject(e);
                }
            });
        });
    }
}

function activate(context) {
    console.log('üöÄ Cursor GPU Mesh integration activated!');
    
    const gpuProvider = new GPUMeshProvider();
    
    // Register commands
    const testCommand = vscode.commands.registerCommand('cursor-gpu-mesh.test', async () => {
        try {
            const response = await gpuProvider.generateResponse("Hello from GPU mesh! Are you working?");
            vscode.window.showInformationMessage(`GPU Mesh: ${response.substring(0, 100)}...`);
        } catch (error) {
            vscode.window.showErrorMessage(`GPU Mesh error: ${error.message}`);
        }
    });
    
    const statusCommand = vscode.commands.registerCommand('cursor-gpu-mesh.status', async () => {
        try {
            const status = await gpuProvider.getGPUMeshStatus();
            vscode.window.showInformationMessage(`GPU Mesh Status: ${status.mesh_status} - ${status.available_vram_gb}GB VRAM`);
        } catch (error) {
            vscode.window.showErrorMessage(`Status error: ${error.message}`);
        }
    });
    
    context.subscriptions.push(testCommand, statusCommand);
}

function deactivate() {
    console.log('üëã Cursor GPU Mesh integration deactivated');
}

module.exports = { activate, deactivate };'''
        
        with open(extension_dir / "extension.js", 'w') as f:
            f.write(extension_js)
        
        print(f"‚úÖ Cursor extension created: {extension_dir}")
        return True
    
    def test_integration(self):
        """Test the complete integration"""
        print("üß™ Testing Cursor GPU mesh integration...")
        
        # Test GPU mesh
        if not self.check_gpu_mesh():
            return False
        
        # Test RunPod Ollama
        if not self.check_runpod_ollama():
            return False
        
        # Test Robbie response
        try:
            payload = {
                "model": "qwen2.5:7b",
                "prompt": "You are Robbie. Say 'GPU mesh turbocharged!' if you can hear me.",
                "stream": False
            }
            
            result = subprocess.run([
                "curl", "-s", "-X", "POST",
                f"{self.runpod_ollama}/api/generate",
                "-H", "Content-Type: application/json",
                "-d", json.dumps(payload)
            ], capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                response = json.loads(result.stdout)
                print(f"‚úÖ Robbie response: {response['response'][:100]}...")
                return True
            else:
                print(f"‚ùå Robbie test failed: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"‚ùå Integration test error: {e}")
            return False
    
    def run_turbocharge(self):
        """Run complete turbocharging process"""
        print("üöÄ TURBOCHARGING CURSOR...")
        
        # Check GPU mesh
        if not self.check_gpu_mesh():
            print("‚ùå GPU mesh not available")
            return False
        
        # Check RunPod Ollama
        if not self.check_runpod_ollama():
            print("‚ùå RunPod Ollama not available")
            return False
        
        # Configure Cursor
        if not self.configure_cursor_for_gpu_mesh():
            print("‚ùå Failed to configure Cursor")
            return False
        
        # Create extension
        if not self.create_cursor_extension():
            print("‚ùå Failed to create extension")
            return False
        
        # Test integration
        if not self.test_integration():
            print("‚ùå Integration test failed")
            return False
        
        print("\n‚úÖ CURSOR TURBOCHARGED!")
        print("=" * 40)
        print("üéØ What's configured:")
        print("   ‚Ä¢ GPU mesh integration (673,780 tokens/min)")
        print("   ‚Ä¢ RunPod Ollama connection")
        print("   ‚Ä¢ Robbie personality system")
        print("   ‚Ä¢ Cursor extension installed")
        print("   ‚Ä¢ Performance optimized")
        print("\nüîÑ Next steps:")
        print("   1. Restart Cursor")
        print("   2. Robbie will use GPU mesh")
        print("   3. 673,780 tokens/min performance")
        print("\nüí∞ Benefits:")
        print("   ‚Ä¢ Maximum AI performance")
        print("   ‚Ä¢ No cloud costs")
        print("   ‚Ä¢ Robbie's full personality")
        print("   ‚Ä¢ Aurora memory integration")
        
        return True

if __name__ == "__main__":
    integration = CursorGPUMeshIntegration()
    success = integration.run_turbocharge()
    
    if not success:
        exit(1)
    
    print("\nüéâ Cursor turbocharged! Restart Cursor to activate.")
CURSOREOF

# Run the integration
python3 /tmp/cursor-gpu-mesh-integration.py

echo ""
echo "üéØ CURSOR TURBOCHARGING COMPLETE!"
echo "================================="
echo ""
echo "‚úÖ GPU Mesh: Connected to RunPod (673,780 tokens/min)"
echo "‚úÖ Ollama: Deployed with GPU acceleration"
echo "‚úÖ Cursor: Configured for maximum performance"
echo "‚úÖ Robbie: Full personality system active"
echo "‚úÖ Extension: GPU mesh integration installed"
echo ""
echo "üîÑ Next steps:"
echo "1. Restart Cursor"
echo "2. Robbie will use GPU mesh for all AI tasks"
echo "3. Enjoy 673,780 tokens/min performance!"
echo ""
echo "üöÄ Cursor is now turbocharged with Aurora AI Empire!"


