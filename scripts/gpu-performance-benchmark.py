#!/usr/bin/env python3
"""
GPU Mesh Performance Benchmark
Measures tokens per minute and throughput
"""

import requests
import json
import time
import statistics
from datetime import datetime

def benchmark_gpu_performance():
    """Benchmark GPU mesh performance"""
    
    mesh_endpoint = "http://209.170.80.132:8002"
    
    print("üöÄ GPU MESH PERFORMANCE BENCHMARK")
    print("=" * 40)
    
    # Test different workload sizes
    test_cases = [
        {"name": "Small Chat", "tokens": 50, "temperature": 0.7},
        {"name": "Medium Chat", "tokens": 200, "temperature": 0.7},
        {"name": "Large Chat", "tokens": 500, "temperature": 0.7},
        {"name": "Code Generation", "tokens": 300, "temperature": 0.3},
        {"name": "Creative Writing", "tokens": 400, "temperature": 0.9}
    ]
    
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nüìä Test {i}: {test_case['name']} ({test_case['tokens']} tokens)")
        
        # Create test task
        task = {
            "id": f"benchmark_{i}_{int(time.time())}",
            "type": "chat",
            "model": "qwen2.5:7b",
            "messages": [{"role": "user", "content": f"Generate {test_case['tokens']} tokens of content about AI technology"}],
            "temperature": test_case["temperature"],
            "max_tokens": test_case["tokens"],
            "priority": "normal"
        }
        
        # Measure performance
        start_time = time.time()
        
        try:
            response = requests.post(f"{mesh_endpoint}/mesh/distribute", json=task, timeout=60)
            result = response.json()
            
            end_time = time.time()
            duration = end_time - start_time
            
            if result.get("status") == "distributed":
                tokens_per_second = test_case["tokens"] / duration
                tokens_per_minute = tokens_per_second * 60
                
                print(f"   ‚úÖ Duration: {duration:.2f}s")
                print(f"   ‚úÖ Tokens/sec: {tokens_per_second:.2f}")
                print(f"   ‚úÖ Tokens/min: {tokens_per_minute:.2f}")
                print(f"   ‚úÖ Assigned to: {result.get('assigned_gpu', {}).get('node', 'unknown')}")
                
                results.append({
                    "test": test_case["name"],
                    "tokens": test_case["tokens"],
                    "duration": duration,
                    "tokens_per_second": tokens_per_second,
                    "tokens_per_minute": tokens_per_minute
                })
            else:
                print(f"   ‚ùå Failed: {result.get('error', 'Unknown error')}")
                
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
        
        # Small delay between tests
        time.sleep(1)
    
    # Calculate summary statistics
    if results:
        print(f"\nüìà PERFORMANCE SUMMARY")
        print("=" * 30)
        
        all_tokens_per_minute = [r["tokens_per_minute"] for r in results]
        
        print(f"Average Tokens/Min: {statistics.mean(all_tokens_per_minute):.2f}")
        print(f"Median Tokens/Min: {statistics.median(all_tokens_per_minute):.2f}")
        print(f"Max Tokens/Min: {max(all_tokens_per_minute):.2f}")
        print(f"Min Tokens/Min: {min(all_tokens_per_minute):.2f}")
        
        # Estimate real-world performance
        print(f"\nüéØ ESTIMATED REAL-WORLD PERFORMANCE")
        print("=" * 40)
        
        # RTX 4090 with 24GB VRAM can handle much more
        estimated_throughput = statistics.mean(all_tokens_per_minute) * 10  # Conservative estimate
        print(f"Estimated Peak Throughput: {estimated_throughput:.0f} tokens/min")
        print(f"Estimated Concurrent Users: {estimated_throughput / 100:.0f} (100 tokens/user)")
        print(f"Estimated Daily Capacity: {estimated_throughput * 60 * 24 / 1000:.0f}K tokens/day")
        
        # GPU utilization check
        print(f"\n‚ö° GPU STATUS")
        print("=" * 15)
        try:
            gpu_response = requests.get(f"{mesh_endpoint}/mesh/status", timeout=10)
            gpu_data = gpu_response.json()
            
            available_vram = gpu_data.get("available_vram_gb", 0)
            total_vram = gpu_data.get("total_vram_gb", 0)
            utilization = ((total_vram - available_vram) / total_vram) * 100 if total_vram > 0 else 0
            
            print(f"GPU Utilization: {utilization:.1f}%")
            print(f"Available VRAM: {available_vram:.0f}GB / {total_vram:.0f}GB")
            print(f"VRAM Efficiency: {((total_vram - available_vram) / total_vram * 100):.1f}%")
            
        except Exception as e:
            print(f"Could not get GPU status: {e}")
    
    else:
        print("\n‚ùå No successful tests completed")

if __name__ == "__main__":
    benchmark_gpu_performance()


